{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"c13df2c91d564a1aa78e68f48864b756":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7667ce90cd4454491ec0818b539b001","IPY_MODEL_b5f51626c4044214b47e96363abbc6f4","IPY_MODEL_4114befafbef45c6baf51ef52f6642a9"],"layout":"IPY_MODEL_00da06a340ba4bfc8b6e8c45b00cc56d"}},"a7667ce90cd4454491ec0818b539b001":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65a05141f737460f888d6cba354d063f","placeholder":"​","style":"IPY_MODEL_c0d4ba2e7953444a9c92aadb430190d3","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"b5f51626c4044214b47e96363abbc6f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_661fb3aa24dc4e8181a7d44ff76d4f94","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5378aaedbe1b44d0aacc9cf6ad4f52e7","value":995526}},"4114befafbef45c6baf51ef52f6642a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a8205941e844c15bc7f5e595728b021","placeholder":"​","style":"IPY_MODEL_1e45fce15ac14d9b9644b190163d4f38","value":" 996k/996k [00:00&lt;00:00, 6.11MB/s]"}},"00da06a340ba4bfc8b6e8c45b00cc56d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65a05141f737460f888d6cba354d063f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0d4ba2e7953444a9c92aadb430190d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"661fb3aa24dc4e8181a7d44ff76d4f94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5378aaedbe1b44d0aacc9cf6ad4f52e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a8205941e844c15bc7f5e595728b021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e45fce15ac14d9b9644b190163d4f38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5055c800bb85466cb2965fb2f0a383a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75a82e8fc9ac45538f0996809ba9e241","IPY_MODEL_4a92e059e4744e398375ac03f59829af","IPY_MODEL_9350bb191d0e4d339f0f55f7754c00cb"],"layout":"IPY_MODEL_7bd708ab1bb64a44b47042c7af6a90c4"}},"75a82e8fc9ac45538f0996809ba9e241":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_598b081ca72846f5896d38098798b802","placeholder":"​","style":"IPY_MODEL_e41e95d75b3d41f88dbe2a65f5864dfc","value":"Downloading (…)okenizer_config.json: 100%"}},"4a92e059e4744e398375ac03f59829af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0239d4059964485974d808204657bff","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39d33a28dc4648e58f9a8136ea4c13aa","value":29}},"9350bb191d0e4d339f0f55f7754c00cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40df52977d264965b7b45d1f4f7aa6d7","placeholder":"​","style":"IPY_MODEL_310787976a374b53aa4ace7a9249c9ff","value":" 29.0/29.0 [00:00&lt;00:00, 703B/s]"}},"7bd708ab1bb64a44b47042c7af6a90c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"598b081ca72846f5896d38098798b802":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e41e95d75b3d41f88dbe2a65f5864dfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0239d4059964485974d808204657bff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d33a28dc4648e58f9a8136ea4c13aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40df52977d264965b7b45d1f4f7aa6d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"310787976a374b53aa4ace7a9249c9ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d96d1aa15df445b1b6fd57d1a5823061":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa34a88f2c53461e85b9ae698c4b6311","IPY_MODEL_2372b72d9dd94590a67a12746d9e46cd","IPY_MODEL_5a36c16bbba94a16939ff7e8111930bc"],"layout":"IPY_MODEL_0a33ca5268ce4ce68ee5007a17597d04"}},"aa34a88f2c53461e85b9ae698c4b6311":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32bd22b3f06b4dc2a91ccac4f729fb71","placeholder":"​","style":"IPY_MODEL_7a5d4d8cca754a8896e489dd48ccbecc","value":"Downloading (…)lve/main/config.json: 100%"}},"2372b72d9dd94590a67a12746d9e46cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03d99b36e7424402a87b1087ba1cbdc1","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ca8796e986b4851a591f21e032c5860","value":625}},"5a36c16bbba94a16939ff7e8111930bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19cfef264eb646c9b49d3a8a1bb6253d","placeholder":"​","style":"IPY_MODEL_d5811cfbf8264e05a4610b97940a45dc","value":" 625/625 [00:00&lt;00:00, 21.0kB/s]"}},"0a33ca5268ce4ce68ee5007a17597d04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32bd22b3f06b4dc2a91ccac4f729fb71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a5d4d8cca754a8896e489dd48ccbecc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03d99b36e7424402a87b1087ba1cbdc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ca8796e986b4851a591f21e032c5860":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19cfef264eb646c9b49d3a8a1bb6253d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5811cfbf8264e05a4610b97940a45dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af7830fd26ea4d989274c6a85d966108":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac661347cf054dfeaf674df03aa7b614","IPY_MODEL_addc28f4faf14004ba6e46ea687b0011","IPY_MODEL_fdb08f1972c24a4fb50f72eb2fc2ae9c"],"layout":"IPY_MODEL_2feed595c2ab4999af4e05f36c3c893f"}},"ac661347cf054dfeaf674df03aa7b614":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52eb1702330246b8928d1d662f0a2ccc","placeholder":"​","style":"IPY_MODEL_70e528cbaca9480da62c144c207186af","value":"Downloading pytorch_model.bin: 100%"}},"addc28f4faf14004ba6e46ea687b0011":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_393d54b79c6a45b6aa6c8185832953ed","max":714314041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f0db159f3a54226a786409b58b6b301","value":714314041}},"fdb08f1972c24a4fb50f72eb2fc2ae9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52f393ed9bd14ec09e9fb3541e93a8cc","placeholder":"​","style":"IPY_MODEL_1bbfb66f0f6e4861b67772983840918e","value":" 714M/714M [00:13&lt;00:00, 52.0MB/s]"}},"2feed595c2ab4999af4e05f36c3c893f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52eb1702330246b8928d1d662f0a2ccc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70e528cbaca9480da62c144c207186af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"393d54b79c6a45b6aa6c8185832953ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f0db159f3a54226a786409b58b6b301":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52f393ed9bd14ec09e9fb3541e93a8cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bbfb66f0f6e4861b67772983840918e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["requirements\n","- transformers : 4.27.1\n","- keras : 2.11.0\n","- tensorflow : 2.11.0\n","- torch : 1.13.1+cu116\n","- pandas : 1.4.4\n","- numpy : 1.22.4"],"metadata":{"id":"lla3kStmuiua"}},{"cell_type":"markdown","source":["# 목적"],"metadata":{"id":"lAx_Z2-OWjHC"}},{"cell_type":"markdown","source":["**- 외국인 관광객의 현재 상태(기분)을 텍스트로 받아 -> Bert를 이용하여 자연어 처리하여 -> 기분에 맞는 음식을 추천해주는 것.**"],"metadata":{"id":"fvmcpqRqWqNh"}},{"cell_type":"markdown","source":["# 데이터 준비"],"metadata":{"id":"pyWBbgt0Wyhv"}},{"cell_type":"markdown","source":["- 캐글에 있는 **Emotion Dataset for Emotion Recognition Tasks** 데이터셋 사용\n","  - 출처: https://www.kaggle.com/datasets/parulpandey/emotion-dataset?select=training.csv\n","- 데이터의 내용은, 텍스트가 6개감정(sad,joy,love,anger,fear,suprise)으로 분류되어 있음.\n","- 데이터는 train,test,val로 되어있고 각각 16000,2000,2000 존재"],"metadata":{"id":"q1zx51jhI9fg"}},{"cell_type":"markdown","source":["# 데이터 전처리"],"metadata":{"id":"1Y7twV5oY1pq"}},{"cell_type":"markdown","source":["- 6개감정(sad,joy,love,anger,fear,suprise)을  -> 3개감정(sad,joy,stress)로 바꿈 (인덱스 번호 각각 0:sad,1:joy,2:stress)\n","  - suprise감정을 지우고 love를 joy와 합쳐서 joy, anger과 fear는 stress로\n","  - 조절한 데이터셋 최종 개수(전처리후) -> train : 17347개 (sad:5216,joy:7548 , stress:4583), test : 1934개 (sad:581,joy:854 , stress:499)\n","- 데이터의 수를 늘리기 위해 val를 train에 합치기"],"metadata":{"id":"uvEu-yy9Y7pg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xaRMvVCKouE"},"outputs":[],"source":["import csv\n","import pandas as pd\n","\n","# 데이터 불러오기\n","# 이부분은 데이터를 각자 경로에 맞게 path를 지정!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","train = pd.read_csv('/content/drive/MyDrive/딥러닝프로젝트/김태혁 최종 폴더/감정분류데이터셋(캐글)/training.csv')\n","test = pd.read_csv('/content/drive/MyDrive/딥러닝프로젝트/김태혁 최종 폴더/감정분류데이터셋(캐글)/test.csv')\n","val = pd.read_csv('/content/drive/MyDrive/딥러닝프로젝트/김태혁 최종 폴더/감정분류데이터셋(캐글)/validation.csv')"]},{"cell_type":"code","source":["train.shape,val.shape,test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BgjLIy3NPCH","executionInfo":{"status":"ok","timestamp":1679190866540,"user_tz":-540,"elapsed":11,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"06a89647-735d-47a0-c2f2-1bdfb6a9ab7f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((16000, 2), (2000, 2), (2000, 2))"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# train과 val합치기\n","train = pd.concat([train,val])\n","train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXpwPUU6NPEV","executionInfo":{"status":"ok","timestamp":1679190866541,"user_tz":-540,"elapsed":9,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"907d2b14-d2f4-48a8-8a5a-7d09adda9ec0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(18000, 2)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# train과 test에서 suprise인5를 제거\n","train = train[train.label != 5]\n","test = test[test.label != 5]\n","train.shape, test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nz2hvwL2THRg","executionInfo":{"status":"ok","timestamp":1679190866541,"user_tz":-540,"elapsed":6,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"d501722a-deff-4027-a3e2-939cf12534e7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((17347, 2), (1934, 2))"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["train.loc[(train['label'] == 2), 'label'] = 1  #love label값인 2를 joy:1 번으로\n","test.loc[(test['label'] == 2), 'label'] = 1 #love label값인 2를 joy:1 번으로\n","\n","train.loc[(train['label'] == 3), 'label'] = 2  #anger label값인 3를 새로운 stress 인덱스 2번으로\n","test.loc[(test['label'] == 3), 'label'] = 2 #anger label값인 3를 새로운 stress 인덱스 2번으로\n","\n","train.loc[(train['label'] == 4), 'label'] = 2  #fear label값인 4를 새로운 stress 인덱스 2번으로\n","test.loc[(test['label'] == 4), 'label'] = 2 #love label값인 2를 새로운 stress 인덱스 2번으로"],"metadata":{"id":"gkZuiQI0REoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train,test에서 결측치 제거와 중복값 제거 수행.(학습에 영향)\n","train.dropna(inplace=True)\n","test.dropna(inplace=True)\n","\n","train.drop_duplicates(subset=['text'], inplace=True)\n","test.drop_duplicates(subset=['text'], inplace=True)\n","\n","train.shape, test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0IoTaH9WLfn","executionInfo":{"status":"ok","timestamp":1679190869388,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"40794033-9c66-43cf-afca-2d04f15e75f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((17316, 2), (1934, 2))"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## 모듈 설치 및 import"],"metadata":{"id":"XYAaJevBaG4j"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tdwpbkb7BRA6","executionInfo":{"status":"ok","timestamp":1679310331839,"user_tz":-540,"elapsed":5438,"user":{"displayName":"11 rookies","userId":"00829008045988138791"}},"outputId":"c1c5982f-4029-419e-ebc2-e773a396fc26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["import transformers\n","transformers.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CALA5VsisJmu","executionInfo":{"status":"ok","timestamp":1679310381115,"user_tz":-540,"elapsed":11,"user":{"displayName":"11 rookies","userId":"00829008045988138791"}},"outputId":"ec7f7b15-c230-48d7-cbda-20bf04947c3c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'4.27.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["!pip install keras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EaIbWjk7MpPu","executionInfo":{"status":"ok","timestamp":1679310507286,"user_tz":-540,"elapsed":4368,"user":{"displayName":"11 rookies","userId":"00829008045988138791"}},"outputId":"db48e941-44b7-4f5e-99fa-e7496d07403d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.11.0)\n"]}]},{"cell_type":"code","source":["import keras\n","keras.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GdRhVc3nt4TK","executionInfo":{"status":"ok","timestamp":1679310707206,"user_tz":-540,"elapsed":7193,"user":{"displayName":"11 rookies","userId":"00829008045988138791"}},"outputId":"a630138b-cfb0-4407-fdf8-7145f0be5d03"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.11.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"metadata":{"id":"qh297tb7MGTv","executionInfo":{"status":"ok","timestamp":1679310762901,"user_tz":-540,"elapsed":5450,"user":{"displayName":"11 rookies","userId":"00829008045988138791"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tf.__version__ , torch.__version__ , pd.__version__, np.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iD5t0x4FuKMH","executionInfo":{"status":"ok","timestamp":1679310971722,"user_tz":-540,"elapsed":3,"user":{"displayName":"11 rookies","userId":"00829008045988138791"}},"outputId":"f589da3c-0df9-489e-f88e-50e0a7cfa6ea"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('2.11.0', '1.13.1+cu116', '1.4.4', '1.22.4')"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"XgjMzosCDD35"},"source":["# **전처리(Bert 입력에 맞게) - 훈련셋**"]},{"cell_type":"code","metadata":{"id":"2GoESQ0jbybJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190896043,"user_tz":-540,"elapsed":10,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"b583ead2-6a52-4b7b-9f38-48f958771847"},"source":["# 리뷰 문장 추출\n","sentences = train['text']\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                               i didnt feel humiliated\n","1     i can go from feeling so hopeless to so damned...\n","2      im grabbing a minute to post i feel greedy wrong\n","3     i am ever feeling nostalgic about the fireplac...\n","4                                  i am feeling grouchy\n","5     ive been feeling a little burdened lately wasn...\n","7     i feel as confused about life as a teenager or...\n","8     i have been with petronas for years i feel tha...\n","9                                   i feel romantic too\n","10    i feel like i have to make the suffering i m s...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"8KkJZvhccRUJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190896044,"user_tz":-540,"elapsed":10,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"e567da3e-4cff-41cd-c50c-6ea58fccac67"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] i didnt feel humiliated [SEP]',\n"," '[CLS] i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake [SEP]',\n"," '[CLS] im grabbing a minute to post i feel greedy wrong [SEP]',\n"," '[CLS] i am ever feeling nostalgic about the fireplace i will know that it is still on the property [SEP]',\n"," '[CLS] i am feeling grouchy [SEP]',\n"," '[CLS] ive been feeling a little burdened lately wasnt sure why that was [SEP]',\n"," '[CLS] i feel as confused about life as a teenager or as jaded as a year old man [SEP]',\n"," '[CLS] i have been with petronas for years i feel that petronas has performed well and made a huge profit [SEP]',\n"," '[CLS] i feel romantic too [SEP]',\n"," '[CLS] i feel like i have to make the suffering i m seeing mean something [SEP]']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"7hBblIVQcXJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190896044,"user_tz":-540,"elapsed":7,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"99486837-2c96-4f64-981e-c579f904362e"},"source":["# 라벨 추출\n","labels = train['label'].values\n","labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 2, ..., 1, 1, 1])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"PwEplfDvcnZG","colab":{"base_uri":"https://localhost:8080/","height":149,"referenced_widgets":["c13df2c91d564a1aa78e68f48864b756","a7667ce90cd4454491ec0818b539b001","b5f51626c4044214b47e96363abbc6f4","4114befafbef45c6baf51ef52f6642a9","00da06a340ba4bfc8b6e8c45b00cc56d","65a05141f737460f888d6cba354d063f","c0d4ba2e7953444a9c92aadb430190d3","661fb3aa24dc4e8181a7d44ff76d4f94","5378aaedbe1b44d0aacc9cf6ad4f52e7","0a8205941e844c15bc7f5e595728b021","1e45fce15ac14d9b9644b190163d4f38","5055c800bb85466cb2965fb2f0a383a1","75a82e8fc9ac45538f0996809ba9e241","4a92e059e4744e398375ac03f59829af","9350bb191d0e4d339f0f55f7754c00cb","7bd708ab1bb64a44b47042c7af6a90c4","598b081ca72846f5896d38098798b802","e41e95d75b3d41f88dbe2a65f5864dfc","a0239d4059964485974d808204657bff","39d33a28dc4648e58f9a8136ea4c13aa","40df52977d264965b7b45d1f4f7aa6d7","310787976a374b53aa4ace7a9249c9ff","d96d1aa15df445b1b6fd57d1a5823061","aa34a88f2c53461e85b9ae698c4b6311","2372b72d9dd94590a67a12746d9e46cd","5a36c16bbba94a16939ff7e8111930bc","0a33ca5268ce4ce68ee5007a17597d04","32bd22b3f06b4dc2a91ccac4f729fb71","7a5d4d8cca754a8896e489dd48ccbecc","03d99b36e7424402a87b1087ba1cbdc1","4ca8796e986b4851a591f21e032c5860","19cfef264eb646c9b49d3a8a1bb6253d","d5811cfbf8264e05a4610b97940a45dc"]},"executionInfo":{"status":"ok","timestamp":1679190906533,"user_tz":-540,"elapsed":10494,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"09e9b514-6098-471f-ee4e-88b42585e0db"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","# 토크나이저는 여러 언어의 데이터를 기반으로 만든 'bert-base-multilingual-cased'를 사용\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c13df2c91d564a1aa78e68f48864b756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5055c800bb85466cb2965fb2f0a383a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d96d1aa15df445b1b6fd57d1a5823061"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[CLS] i didnt feel humiliated [SEP]\n","['[CLS]', 'i', 'didn', '##t', 'feel', 'hu', '##mil', '##iated', '[SEP]']\n"]}]},{"cell_type":"code","metadata":{"id":"VJ76KiP_dLn-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190907676,"user_tz":-540,"elapsed":1147,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"a2c70f94-59ad-45a0-8274-f8cfecf8f98f"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  101,   177, 34420, 10123, 38008, 26506, 55177, 89771,   102,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"pKfL8SotdVaW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190915023,"user_tz":-540,"elapsed":7349,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"f08db7a8-3947-4c78-b43a-77f897a2921e"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"]}]},{"cell_type":"code","metadata":{"id":"1f5Vq3-7eNKH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190915687,"user_tz":-540,"elapsed":666,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"e027ee6a-2201-4a78-df0c-66cfa1b2dae6"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([  101,   177, 68507, 17761,   169, 19826, 10111,   177, 19556, 15127,\n","        27925, 10135, 10435, 20363, 10107, 10108, 40421, 10473, 10992, 10108,\n","        10105, 38576, 10211, 61362, 18745, 10944, 10347, 21484, 83865, 10135,\n","        23582, 10108, 63658, 10111, 10105, 19573, 11951, 18571, 59381, 12166,\n","        10160, 83018,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n","tensor([  101,   177, 38008, 11850, 10531, 13920, 11337, 24376, 15217, 10911,\n","        36833,   169, 69974, 13000, 10189, 10124, 10104, 25907, 34279, 10146,\n","        11206, 10146, 65763, 10114, 15127, 14234, 24558, 11760,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(1)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"]}]},{"cell_type":"code","metadata":{"id":"I3vlyUJuVRo5"},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkqUHx51dffp"},"source":["# **전처리 - 테스트셋**"]},{"cell_type":"code","metadata":{"id":"xgrsNuArd4pj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190915688,"user_tz":-540,"elapsed":25,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"e9ae1f37-5525-42e9-9e98-4daeeccc1340"},"source":["# 리뷰 문장 추출\n","sentences = test['text']\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    im feeling rather rotten so im not very ambiti...\n","1            im updating my blog because i feel shitty\n","2    i never make her separate from me because i do...\n","3    i left with my bouquet of red and yellow tulip...\n","4      i was feeling a little vain when i did this one\n","5    i cant walk into a shop anywhere where i do no...\n","6     i felt anger when at the end of a telephone call\n","7    i explain why i clung to a relationship with a...\n","8    i like to have the same breathless feeling as ...\n","9    i jest i feel grumpy tired and pre menstrual w...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Gtz3QZt9d4pz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190915688,"user_tz":-540,"elapsed":16,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"59757043-e6a7-4100-edcb-8cbeb0007a80"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] im feeling rather rotten so im not very ambitious right now [SEP]',\n"," '[CLS] im updating my blog because i feel shitty [SEP]',\n"," '[CLS] i never make her separate from me because i don t ever want her to feel like i m ashamed with her [SEP]',\n"," '[CLS] i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived [SEP]',\n"," '[CLS] i was feeling a little vain when i did this one [SEP]',\n"," '[CLS] i cant walk into a shop anywhere where i do not feel uncomfortable [SEP]',\n"," '[CLS] i felt anger when at the end of a telephone call [SEP]',\n"," '[CLS] i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia [SEP]',\n"," '[CLS] i like to have the same breathless feeling as a reader eager to see what will happen next [SEP]',\n"," '[CLS] i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer [SEP]']"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"li8oRajbd4p3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190915689,"user_tz":-540,"elapsed":12,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"7197cc33-bbff-43fc-c40b-b1ca661b8e3d"},"source":["# 라벨 추출\n","labels = test['label'].values\n","labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 1, 1, 2])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"lvpQ49nEd4p6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190918847,"user_tz":-540,"elapsed":3165,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"054083c9-dbf6-46e3-8463-211e8a3176fb"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] im feeling rather rotten so im not very ambitious right now [SEP]\n","['[CLS]', 'im', 'feeling', 'rather', 'rot', '##ten', 'so', 'im', 'not', 'very', 'amb', '##iti', '##ous', 'right', 'now', '[SEP]']\n"]}]},{"cell_type":"code","metadata":{"id":"HI9viuAvd4p_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190919179,"user_tz":-540,"elapsed":336,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"c82fae2f-b25f-4a95-8963-775c600658d4"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  101, 10211, 61362, 16863, 64354, 10681, 10380, 10211, 10472,\n","       12558, 10559, 13903, 13499, 13448, 11858,   102,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"v1NKmP0Fd4qD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190919873,"user_tz":-540,"elapsed":696,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"502c7564-0d9d-485f-90ad-ea8e5b12f40d"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"]}]},{"cell_type":"code","metadata":{"id":"RIkaYCGbd4qG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190919874,"user_tz":-540,"elapsed":6,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"957c3b39-a606-4d40-ee69-05f8e3093764"},"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([  101, 10211, 61362, 16863, 64354, 10681, 10380, 10211, 10472, 12558,\n","        10559, 13903, 13499, 13448, 11858,   102,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"]}]},{"cell_type":"code","metadata":{"id":"7gwdYv1Ad4qK"},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBvpU-Hfgcth"},"source":["<br>\n","<br>\n","\n","# **모델 생성**"]},{"cell_type":"code","metadata":{"id":"heToD1ev0mOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190924953,"user_tz":-540,"elapsed":5082,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"3316fea1-2b41-4c4c-83eb-927c965a00e9"},"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"id":"f6enIxvt1FB2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190934586,"user_tz":-540,"elapsed":8471,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"20dc88a7-a58a-41ef-a835-e3ca4b475d5c"},"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"code","metadata":{"id":"MS2MXSiLg5zC","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["af7830fd26ea4d989274c6a85d966108","ac661347cf054dfeaf674df03aa7b614","addc28f4faf14004ba6e46ea687b0011","fdb08f1972c24a4fb50f72eb2fc2ae9c","2feed595c2ab4999af4e05f36c3c893f","52eb1702330246b8928d1d662f0a2ccc","70e528cbaca9480da62c144c207186af","393d54b79c6a45b6aa6c8185832953ed","5f0db159f3a54226a786409b58b6b301","52f393ed9bd14ec09e9fb3541e93a8cc","1bbfb66f0f6e4861b67772983840918e"]},"executionInfo":{"status":"ok","timestamp":1679190951538,"user_tz":-540,"elapsed":16955,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"9bbc1396-f308-4ab9-ae46-096561e6fe5a"},"source":["# 분류를 위한 BERT 모델 생성, 분류 labels 는 3개\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=3)\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7830fd26ea4d989274c6a85d966108"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"ZIdfbLTuWmxk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679190951538,"user_tz":-540,"elapsed":41,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"664ccf43-4944-4682-c842-8f1c0cd7033a"},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 4\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","metadata":{"id":"gzCHV_ghj7DM"},"source":["<br>\n","<br>\n","\n","# **모델 학습**"]},{"cell_type":"code","metadata":{"id":"S0-p6pPVXCRe"},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJXISnJzCdLM"},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muU2kS2GCh4y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fb70fd6-70f4-47e5-e60e-a8fcce5eafe8","executionInfo":{"status":"ok","timestamp":1679192311125,"user_tz":-540,"elapsed":1359620,"user":{"displayName":"김태혁","userId":"17428339584721718010"}}},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 출력 로짓 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.39\n","  Training epcoh took: 0:05:31\n","\n","Running Validation...\n","  Accuracy: 0.97\n","  Validation took: 0:00:12\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:05:27\n","\n","Running Validation...\n","  Accuracy: 0.97\n","  Validation took: 0:00:12\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.06\n","  Training epcoh took: 0:05:27\n","\n","Running Validation...\n","  Accuracy: 0.98\n","  Validation took: 0:00:12\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.03\n","  Training epcoh took: 0:05:27\n","\n","Running Validation...\n","  Accuracy: 0.98\n","  Validation took: 0:00:12\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["# 테스트 들어가기 전 정리\n","\n","- 디바이스 : GPU\n","- 모델 : bert-base-multilingual-cased\n","- 옵티마이저 : Adam\n","- epochs : 4\n","- batch_size : 32\n","- 학습시간 : 25분\n"],"metadata":{"id":"2HcZ8VoxqJFE"}},{"cell_type":"markdown","metadata":{"id":"6BVbl4Zjatzn"},"source":["# **테스트셋 평가**"]},{"cell_type":"code","metadata":{"id":"c5KHb6RkbHdj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679192324464,"user_tz":-540,"elapsed":13363,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"ece7bd04-0e2a-4d11-b3d6-b30446cc3a58"},"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 출력 로짓 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.98\n","Test took: 0:00:13\n"]}]},{"cell_type":"markdown","source":["# 모델 저장 및 로드"],"metadata":{"id":"jcYJGqT7hFHT"}},{"cell_type":"code","source":["path = '/content/drive/MyDrive/Cloud_AI/감정모델bert저장/'\n","torch.save(model, path + 'bert_final_text_model.pt')  # 전체 모델 저장"],"metadata":{"id":"tAaICsFTbVnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#★★★현재경로가 model이 있는 폴더여야함★★★\n","import os\n","import torch\n","!pip install transformers # 이것이 깔려야 로델이 불려옴.\n","os.chdir('/content/drive/MyDrive/Cloud_AI/감정모델bert저장/')\n","\n","device = torch.device(\"cpu\")\n","model1 = torch.load('bert_final_text_model.pt',map_location=device) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlS7FmRyiCtY","executionInfo":{"status":"ok","timestamp":1679192425302,"user_tz":-540,"elapsed":28129,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"226731a2-b427-4f10-e3da-907eaa066f29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.27.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"U7SzL1IBe1Dm"},"source":["<br>\n","<br>\n","\n","# **새로운 문장 테스트**"]},{"cell_type":"code","metadata":{"id":"Tb4v_VfEfGQB"},"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C12NL1Fvgv4E"},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model1.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model1(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 출력 로짓 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQezr0tljJlM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679192548683,"user_tz":-540,"elapsed":1330,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"aadaabd2-087e-4a76-a5d1-87706162ac98"},"source":["# 로드 모델이 돌아가기위한 세팅 및 실험.\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","\n","logits = test_sentences(['i hate it when i feel fearful for absolutely no reason'])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-2.4447374 -2.8480904  5.5362806]]\n","2\n"]}]},{"cell_type":"markdown","source":["# 학습시킨 Bert모델을 이용한 감정분류를 통한 음식 추천."],"metadata":{"id":"MGk67lHXrYAi"}},{"cell_type":"markdown","source":["- 기분 별로 음식을 나눈 기준은 학술논문 **'대학생들의 정서에 따른 컴포트 푸드의 차이:성차를 중심으로'**를 참조하여서 분류하였습니다."],"metadata":{"id":"6LOCEdGdrihR"}},{"cell_type":"code","source":["import random\n","\n","good= [\"Grilled Ribs\", \"Yukhoe\", \"Steamed Ribs\", \"Grilled Ribs\", \"Grilled Tripe\", \"Grilled Tripe Hot Pot\"]\n","\n","\n","sad=['Cold Raw Fish', 'Grilled Pollack', 'Grilled Eel', 'Grilled Chopper', 'Grilled Shellfish', 'Seaweed Soup', 'Fried Squid', 'Fried Shrimp'\n",", \"Seaweed\", \"Sannakji\", \"Seasoned raw octopus\", \"Seaweed\", \"Shrimp fried rice\", \"Stir-fried webfoot octopus\", \"Seasoned crab\", \"Fish pancake\", \"Steamed pollack\",\n","\"Braised saury,\" \"Dongtae-jjigae,\" \"Steamed seafood,\" \"Seasoned chicken,\" \"Jajangmyeon,\" \"Jjolmyeon,\" \"Kongguksu,\" \"Rice balls,\" \"Japchae.\"\n",", \"Yubu Sushi\", \"Rice Skewers\", \"Pumpkin Jeon\", \"Soy sauce marinated crab\", \"Grilled hairtail\", \"Grilled mackerel\", \"Steamed mackerel\", \"Gwamegi\"]\n","\n","\n","stressful=['grilled pollack', 'spicy stir-fried chicken', 'spicy stew', 'jjolmyeon', 'yukgaejang', 'bibim naengmyeon', 'sushi salad', 'skirt salad', 'tofu kimchi'\n","\"Stir-fried spicy pork\", \"Stir-fried webfoot octopus\", \"Tteokbokki\", \"Rapokki\", \"Seasoned crab\", \"Stir-fried chicken\", \"Steamed pollack\", \"Dong7tae jjigae\", \"Steamed seafood\",\n","\"Cold Noodles\", \"Kongguksu\", \"Boiled Potatoes\", \"Fried Chili\", \"Kimchi Pancake\"]"],"metadata":{"id":"tWYJi2cUiXl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["end = 1  \n","while end == 1:\n","    input_sent = input(\"Please enter what you want to say : \")\n","    if input_sent == '0':\n","        break\n","    #input_list = list(input_sent)\n","    logits = test_sentences([input_sent])\n","    #0print(logits)\n","\n","    if np.argmax(logits) == 0: # 만약에 분류감정이 sad면\n","        select_food = random.choice(sad)\n","        print(f'You look sad.') \n","        print(f'{select_food} is the best when you are depressed.')\n","    elif np.argmax(logits) == 1: # 만약에 분류감정이 happy면\n","        select_food = random.choice(good)\n","        print(f'You must be in a good mood. Hoho! I recommend you to eat when you feel good!')\n","        print(f'My choice is {select_food}.')\n","    elif np.argmax(logits) == 2: # 만약에 분류감정이 stress면\n","        select_food = random.choice(stressful)\n","        print(f'You look stressed!')\n","        print(f'{select_food} is the best when you are stressed.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekZ9DLYWidrt","executionInfo":{"status":"ok","timestamp":1679192797094,"user_tz":-540,"elapsed":50447,"user":{"displayName":"김태혁","userId":"17428339584721718010"}},"outputId":"4ae10787-f488-41d3-e68a-5e71536b5d7c"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Please enter what you want to say : i have been with petronas for years i feel that petronas has performed well and made a huge profit\n","You must be in a good mood. Hoho! I recommend you to eat when you feel good!\n","My choice is Grilled Tripe.\n","Please enter what you want to say : i feel like i have to make the suffering i m seeing mean something\n","You look sad.\n","Stir-fried webfoot octopus is the best when you are depressed.\n","Please enter what you want to say : i now feel compromised and skeptical of the value of every unit of work i put in\n","You look stressed!\n","Steamed seafood is the best when you are stressed.\n","Please enter what you want to say : 0\n"]}]},{"cell_type":"markdown","source":["- 위의 결과를 보면 영어 텍스트로 입력했을때, 감정을 잘 분류하는 것으로 나옴."],"metadata":{"id":"x1TMyLEXsLXS"}}]}